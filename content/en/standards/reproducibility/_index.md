---
title: Reproducibility
linkTitle: Reproducibility
cascade:
  type: reproducibility
  github_issue_filter: "label:\"standards - reproducibility\""
  github_issue_template: reproducibility.md
weight: 2
---

{{% standards-preamble %}}
These standards promote the reuse of model code and reproducibility of model results.
{{% /standards-preamble %}}

In this document we are adopting the [National Academies of Sciences, Engineering and Medicine 2019 report on Reproducibility and Replicability in Science](https://doi.org/10.17226/25303) definitions of reproducibility and replicability.

> Reproducibility means obtaining consistent computational results using the same input data, computational steps, methods, code, and conditions of analysis.  Replicability means obtaining consistent results across studies aimed at answering the same scientific question, each of which has obtained its own data.  

Reproducibility is a cornerstone of scientific research, especially for transparent evaluation of research claims. Equally important is the potential for scientific procedures to be reused and modified in order to carry out new research. While clear documentation is essential to reusability, new technologies also offer the potential to increase the reproducibility and reusability of model code. A critical metric of reproducibility is whether a model can consistently reproduce the published results that claim to have been generated by the model. Reproducibility and reusability can also be enhanced if model code is accompanied by explicit workflows that demonstrate how the model generates and transforms its output data into its published findings.

### Minimal Standards 

This set of minimal standards can be easily adopted by journals to ensure that submitted publications meet baseline reproducibility and reusability requirements. The high level goal is for an independent reviewer to be able to easily download, build, and execute a computational model and verify that it meets the reproducibility claims stated by the model author(s). 

Computational models must:

- meet OMF minimal standards for <a href='{{< relref "/standards/accessibility" >}}'>Accessibility</a> and <a href='{{< relref "/standards/documentation" >}}'>Documentation</a>
- include a minimal set of metadata describing the model including: name, version, description, authors and contributors, [OSI approved license](https://choosealicense.com/), a permanent link to a versioned codebase landing page that broadly follows the [FORCE11 software citation guidelines](https://www.force11.org/software-citation-principles)
- include provenance of all input datasets: permanent links to versioned input data dependencies (and outputs where possible)
- favor open file formats for data inputs and outputs (e.g., CSV, netCDF, YAML/JSON)
- provide information on all versioned software and system dependencies (operating system, software and system libraries) with relevant software citations for key dependencies
- include instructions on how to set up a compute environment to compile / interpret / execute the software
  - software with large compute or data requirements can be problematic:
    - provide representative input data samples along with sampling methodology
    - provide durable link to wholetale or other provenance-tracked computation
- clearly describe what reproducibility measures or metrics are applicable to the given software
- provide range of acceptable outcomes for software with stochastic components
- provide descriptions of possible input parameters and expected outputs with units and format (e.g., shape, data type, etc.)
- include output analyses and workflows
  - scripts to transform raw data -> intermediate -> figures
  - subsets of intermediate data if prohibitive to generate

### Ideal Standards

In order to meet the ideal standards, computational models should:

- provide durable containerization that follows archival best practices with archived container images
- provide a link to an active source code repository if still in active development
- have continuous integration services that compile and run automated unit or integration test suites on a periodic basis
- include metadata on related research outputs (publications, other software, relationship)
- additional domain specific standards if any (examples?)

### Examples / Exemplars / References

- [Lorena Barba's reproducible workflow for computational fluid dynamics](https://doi.org/10.5281/zenodo.2642710) https://github.com/barbagroup/cloud-repro 
- https://www.practicereproducibleresearch.org/
- https://reproduciblescience.org/directory/
- [Software Deposit Guidelines from SSI](https://softwaresaved.github.io/software-deposit-guidance/HowToDescribeSoftwareDeposit.html)
- [Proposed Standards for Peer-Reviewed Publication of Computer Code](https://doi.org/10.2134/agronj2015.0481)
- find example codebases that meet minimal and ideal standards 

### Tools

Build Docker images from research code:
- stencila/dockta https://github.com/stencila/dockta
- ReproZip https://www.reprozip.org/
- SciUnit https://sciunit.run/
- binder https://mybinder.org/
- repo2docker https://repo2docker.readthedocs.io (used by binder)

Archive computational pipelines:
- [Whole Tale](https://wholetale.org/)
- [Code Ocean](https://codeocean.com/)

OMF may consider building some github template repositories or scaffolding for common modeling frameworks that reduce friction of adoption.
  - e.g., https://github.com/uwescience/shablona and https://github.com/geodynamics/software_template
  - a GitHub bot that submits PRs against a GitHub repository to improve compliance with minimal / ideal standards
  - e.g. provide a cookiecutter project structure that supports best practices for reproducibility and reusability like [Cookiecutter Data Science](http://drivendata.github.io/cookiecutter-data-science/) 

### Issues / Errata

- It is fine to use commercial and / or closed source products so long as the researchers' source code is available and the version of the product is specified (e.g., matlab 9.5, AnyLogic 8.4, ArcGIS 10.8)
